{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2a7c84",
   "metadata": {},
   "source": [
    "# How to Build Time Series Applications in CrateDB\n",
    "\n",
    "This notebook guides you through an example of how to import and work with\n",
    "time series data in CrateDB. It uses Dask to import data into CrateDB.\n",
    "Dask is a framework to parallelize operations on pandas Dataframes.\n",
    "\n",
    "## Dataset\n",
    "This notebook uses a daily weather data set provided on kaggle.com. This dataset\n",
    "offers a collection of **daily weather readings from major cities around the\n",
    "world, making up to ~1250 cities**. Some locations provide historical data\n",
    "tracing back to 1833, giving users a deep dive into **long-term weather patterns\n",
    "and their evolution**.\n",
    "\n",
    "The measurements include a few time series, listed here:\n",
    "\n",
    "- Station ID\n",
    "- City Name\n",
    "- Timestamp (granularity: day)\n",
    "- Season\n",
    "- Average temperature in °C\n",
    "- Minimum temperature in °C\n",
    "- Maximum temperature in °C\n",
    "- Precipitation in mm\n",
    "- Snow depth in mm\n",
    "- Average wind direction in degrees\n",
    "- Average wind speed in km/h\n",
    "- Peak wind gust in km/h\n",
    "- Average sea level pressure in hpa\n",
    "- Total sunshine in min\n",
    "\n",
    "The data set is available on Kaggle at [The Weather Dataset].\n",
    "\n",
    "[The Weather Dataset]: https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cf879",
   "metadata": {},
   "source": [
    "## Step 1: Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!pip install dask pandas 'crate[sqlalchemy]'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a31d75fa072055fe"
  },
  {
   "cell_type": "markdown",
   "id": "4aa9b1d0",
   "metadata": {},
   "source": [
    "## Step 2: Read and prepare the data\n",
    "\n",
    "Download and prepare the data for importing into CrateDB.\n",
    "\n",
    "The following data sets need to be processed:\n",
    "- Daily weather data (daily_weather.parquet)\n",
    "- Cities (cities.csv)\n",
    "- Countries (countries.csv)\n",
    "\n",
    "The subsequent code cell acquires the dataset directly from kaggle.com.\n",
    "To properly configure the notebook to use corresponding credentials\n",
    "after signing up on Kaggle, define the `KAGGLE_USERNAME` and\n",
    "`KAGGLE_KEY` environment variables. Alternatively, put them into the\n",
    "file `~/.kaggle/kaggle.json` in your home folder, like this:\n",
    "```json\n",
    "{\n",
    "  \"username\": \"acme\",\n",
    "  \"key\": \"2b1dac2af55caaf1f34df76236fada4a\"\n",
    "}\n",
    "```\n",
    "Another variant is to acquire the dataset files manually, and extract\n",
    "them into a folder called `DOWNLOAD`. In this case, you can deactivate\n",
    "those two lines of code, in order to skip automatic dataset acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data\n"
     ]
    }
   ],
   "source": [
    "from cratedb_toolkit.datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"kaggle://guillemservera/global-daily-climate-data/daily_weather.parquet\")\n",
    "dataset.acquire()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:23.278771Z",
     "start_time": "2024-04-08T15:42:22.381931Z"
    }
   },
   "id": "dd108f22d04130e4"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Show a progress bar for dask activities\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:23.358987Z",
     "start_time": "2024-04-08T15:42:23.356811Z"
    }
   },
   "id": "56672457911d1bea"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a506f7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:30.181619Z",
     "start_time": "2024-04-08T15:42:24.164528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 3.22 ss\n",
      "[########################################] | 100% Completed | 3.32 s\n",
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Int64Index: 27635763 entries, 0 to 24220\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype\n",
      "---  ------                  --------------  -----\n",
      " 0   station_id              27635763 non-null      category\n",
      " 1   city_name               27621770 non-null      category\n",
      " 2   date                    27635763 non-null      datetime64[ns]\n",
      " 3   season                  27635763 non-null      category\n",
      " 4   avg_temp_c              21404856 non-null      float64\n",
      " 5   min_temp_c              21917534 non-null      float64\n",
      " 6   max_temp_c              22096417 non-null      float64\n",
      " 7   precipitation_mm        20993263 non-null      float64\n",
      " 8   snow_depth_mm           3427148 non-null      float64\n",
      " 9   avg_wind_dir_deg        3452568 non-null      float64\n",
      "10   avg_wind_speed_kmh      5285468 non-null      float64\n",
      "11   peak_wind_gust_kmh      1121486 non-null      float64\n",
      "12   avg_sea_level_pres_hpa  4017157 non-null      float64\n",
      "13   sunshine_total_min      1021461 non-null      float64\n",
      "dtypes: category(3), datetime64[ns](1), float64(10)\n",
      "memory usage: 2.6 GB\n",
      "[########################################] | 100% Completed | 2.56 ss\n",
      "[########################################] | 100% Completed | 2.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "  station_id city_name       date  season  avg_temp_c  min_temp_c  max_temp_c  \\\n0      41515  Asadabad 1957-07-01  Summer        27.0        21.1        35.6   \n1      41515  Asadabad 1957-07-02  Summer        22.8        18.9        32.2   \n2      41515  Asadabad 1957-07-03  Summer        24.3        16.7        35.6   \n3      41515  Asadabad 1957-07-04  Summer        26.6        16.1        37.8   \n4      41515  Asadabad 1957-07-05  Summer        30.8        20.0        41.7   \n\n   precipitation_mm  snow_depth_mm  avg_wind_dir_deg  avg_wind_speed_kmh  \\\n0               0.0            NaN               NaN                 NaN   \n1               0.0            NaN               NaN                 NaN   \n2               1.0            NaN               NaN                 NaN   \n3               4.1            NaN               NaN                 NaN   \n4               0.0            NaN               NaN                 NaN   \n\n   peak_wind_gust_kmh  avg_sea_level_pres_hpa  sunshine_total_min  \n0                 NaN                     NaN                 NaN  \n1                 NaN                     NaN                 NaN  \n2                 NaN                     NaN                 NaN  \n3                 NaN                     NaN                 NaN  \n4                 NaN                     NaN                 NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>station_id</th>\n      <th>city_name</th>\n      <th>date</th>\n      <th>season</th>\n      <th>avg_temp_c</th>\n      <th>min_temp_c</th>\n      <th>max_temp_c</th>\n      <th>precipitation_mm</th>\n      <th>snow_depth_mm</th>\n      <th>avg_wind_dir_deg</th>\n      <th>avg_wind_speed_kmh</th>\n      <th>peak_wind_gust_kmh</th>\n      <th>avg_sea_level_pres_hpa</th>\n      <th>sunshine_total_min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>1957-07-01</td>\n      <td>Summer</td>\n      <td>27.0</td>\n      <td>21.1</td>\n      <td>35.6</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>1957-07-02</td>\n      <td>Summer</td>\n      <td>22.8</td>\n      <td>18.9</td>\n      <td>32.2</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>1957-07-03</td>\n      <td>Summer</td>\n      <td>24.3</td>\n      <td>16.7</td>\n      <td>35.6</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>1957-07-04</td>\n      <td>Summer</td>\n      <td>26.6</td>\n      <td>16.1</td>\n      <td>37.8</td>\n      <td>4.1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>1957-07-05</td>\n      <td>Summer</td>\n      <td>30.8</td>\n      <td>20.0</td>\n      <td>41.7</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parquet file. Please adjust the file path as needed.\n",
    "df_kaggle = dd.read_parquet('DOWNLOAD/daily_weather.parquet')\n",
    "\n",
    "# Show info about the data.\n",
    "df_kaggle.info(verbose=True, memory_usage=True)\n",
    "\n",
    "# Display the first rows.\n",
    "df_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c083721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:30.407165Z",
     "start_time": "2024-04-08T15:42:30.182080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 101.33 ms\n",
      "[########################################] | 100% Completed | 201.26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "  station_id   city_name      country       state iso2 iso3  \\\n0      41515    Asadabad  Afghanistan       Kunar   AF  AFG   \n1      38954    Fayzabad  Afghanistan  Badakhshan   AF  AFG   \n2      41560   Jalalabad  Afghanistan   Nangarhar   AF  AFG   \n3      38947      Kunduz  Afghanistan      Kunduz   AF  AFG   \n4      38987  Qala i Naw  Afghanistan     Badghis   AF  AFG   \n\n                              loc  \n0  [71.1500045859, 34.8660000397]  \n1  [70.5792471913, 37.1297607616]  \n2  [70.4361034738, 34.4415269155]  \n3  [68.8725296619, 36.7279506623]  \n4   [63.1332996367, 34.983000131]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>station_id</th>\n      <th>city_name</th>\n      <th>country</th>\n      <th>state</th>\n      <th>iso2</th>\n      <th>iso3</th>\n      <th>loc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41515</td>\n      <td>Asadabad</td>\n      <td>Afghanistan</td>\n      <td>Kunar</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>[71.1500045859, 34.8660000397]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38954</td>\n      <td>Fayzabad</td>\n      <td>Afghanistan</td>\n      <td>Badakhshan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>[70.5792471913, 37.1297607616]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41560</td>\n      <td>Jalalabad</td>\n      <td>Afghanistan</td>\n      <td>Nangarhar</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>[70.4361034738, 34.4415269155]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>38947</td>\n      <td>Kunduz</td>\n      <td>Afghanistan</td>\n      <td>Kunduz</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>[68.8725296619, 36.7279506623]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38987</td>\n      <td>Qala i Naw</td>\n      <td>Afghanistan</td>\n      <td>Badghis</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>[63.1332996367, 34.983000131]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read cities, adapt the path to the files accordingly\n",
    "cities = dd.read_csv(\"DOWNLOAD/cities.csv\",dtype={'station_id': 'object'})\n",
    "\n",
    "# Modify lon and lat of cities into an array that can be interpreted directly by CrateDB\n",
    "def create_location_column(df):\n",
    "    df['loc'] = df[['longitude', 'latitude']].values.tolist()\n",
    "    return df\n",
    "\n",
    "cities = cities.map_partitions(create_location_column)\n",
    "cities = cities.drop(['longitude', 'latitude'], axis=1)\n",
    "\n",
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "903e0fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:31.545551Z",
     "start_time": "2024-04-08T15:42:31.542164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read countries, adapt the path to the files accordingly\n",
    "countries = dd.read_csv(\"DOWNLOAD/countries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cd33c",
   "metadata": {},
   "source": [
    "## Step 3: Import data into CrateDB\n",
    "\n",
    "After acquiring and preparing data files and data frames, they can be imported\n",
    "into CrateDB. In order to provide the correct datatypes, and use, for example,\n",
    "fulltext indexes, the tables are created manually. When writing a dataframe to\n",
    "CrateDB, the schema can also be derived automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect to CrateDB\n",
    "\n",
    "This code uses SQLAlchemy to connect to CrateDB."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f1b9a28612a491"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eaf4af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:33.584727Z",
     "start_time": "2024-04-08T15:42:33.575660Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlalchemy as sa\n",
    "from crate.client.sqlalchemy.support import insert_bulk\n",
    "\n",
    "# Define database address when using CrateDB Cloud.\n",
    "# Please find these settings on your cluster overview page.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://<USER>:<PASSWORD>@<CRATEDB_HOST>/?ssl=true\",\n",
    ")\n",
    "\n",
    "# Define database address when using CrateDB on localhost.\n",
    "#CONNECTION_STRING = os.environ.get(\n",
    "#    \"CRATEDB_CONNECTION_STRING\",\n",
    "#    \"crate://crate@localhost/\",\n",
    "#)\n",
    "\n",
    "# Connect to CrateDB using SQLAlchemy.\n",
    "engine = sa.create_engine(CONNECTION_STRING, echo=False)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee5dab",
   "metadata": {},
   "source": [
    "#### Create tables\n",
    "\n",
    "Now let's create the weather data table. We want to use fulltext search capabilities\n",
    "on the city name, thus there is a corresponding an index defined on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f972876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:34.798906Z",
     "start_time": "2024-04-08T15:42:34.793221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x794d301630d0>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection.execute(sa.text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS \"weather_data\" (\n",
    "   \"station_id\" TEXT,\n",
    "   \"city_name\" TEXT,\n",
    "   \"date\" TIMESTAMP WITHOUT TIME ZONE,\n",
    "   \"season\" TEXT,\n",
    "   \"avg_temp_c\" REAL,\n",
    "   \"min_temp_c\" REAL,\n",
    "   \"max_temp_c\" REAL,\n",
    "   \"precipitation_mm\" REAL,\n",
    "   \"snow_depth_mm\" REAL,\n",
    "   \"avg_wind_dir_deg\" REAL,\n",
    "   \"avg_wind_speed_kmh\" REAL,\n",
    "   \"peak_wind_gust_kmh\" REAL,\n",
    "   \"avg_sea_level_pres_hpa\" REAL,\n",
    "   \"sunshine_total_min\" REAL,\n",
    "   INDEX city_name_ft using fulltext (city_name)\n",
    ")\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9021224",
   "metadata": {},
   "source": [
    "Now, create the `cities` table using a `GEO_POINT` column to store location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d8b46de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:43.972291Z",
     "start_time": "2024-04-08T15:42:43.880509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x794cf4112a50>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection.execute(sa.text(\"\"\"\n",
    "CREATE TABLE \"cities\" (\n",
    "   \"station_id\" TEXT,\n",
    "   \"city_name\" TEXT,\n",
    "   \"country\" TEXT,\n",
    "   \"state\" TEXT,\n",
    "   \"iso2\" TEXT,\n",
    "   \"iso3\" TEXT,\n",
    "   \"loc\" GEO_POINT\n",
    ")\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e31e1",
   "metadata": {},
   "source": [
    "#### Import Weather Data\n",
    "\n",
    "If you are using a **CrateDB Cloud cluster**, the easiest and fastest way to\n",
    "import the data is to use the **import mechanism of CrateDB Cloud**. It avoids\n",
    "to transfer a lot of data across the network, as the Parquet file is uploaded\n",
    "directly into a staging area and imported into CrateDB.\n",
    "\n",
    "If you are running **CrateDB locally**, or do not want to use the GUI, we recommend\n",
    "to use a parallelized import via Dask, which follows a few relevant ideas.\n",
    "Background: pandas data frames would only use one CPU to prepare the data and not\n",
    "utilize the database good enough.\n",
    "\n",
    "- Create additional partitions to parallelize the import to CrateDB.\n",
    "  They will be automatically processed/imported in parallel by Dask.\n",
    "- Tuning the concurrency and batch size parameters correctly is important to not\n",
    "  overload the database. A chunk size of 10,000 has shown good results on a single\n",
    "  CrateDB node with 4 GB of assigned heap memory.\n",
    "  Please watch the logs on the insert operation: If the garbage collector consumes\n",
    "  a lot of time, it is an indicator that there is not enough memory assigned to\n",
    "  CrateDB's heap.\n",
    "- Instead of individual `INSERT` statements, the method outlined above uses the\n",
    "  \"bulk insert method\" of CrateDB.\n",
    "- The parallelization of the import procedure works on all partitions.\n",
    "- Running CrateDB in a local Docker container with 5 assigned CPUs, 8 GB total memory,\n",
    "  and 4 GB heap space, led to about 80,000 inserts/second, including all the indexing.\n",
    "\n",
    "You can find additional hints about importing large datasets via Python's Dask Data\n",
    "Frames to CrateDB at [SQLAlchemy: DataFrame operations].\n",
    "\n",
    "[SQLAlchemy: DataFrame operations]: https://cratedb.com/docs/python/en/latest/by-example/sqlalchemy/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311e588c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:45.199060Z",
     "start_time": "2024-04-08T15:42:45.197376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines to process the actual weather data.\n",
    "# They have been disabled in order to avoid long-running operations.\n",
    "# df_kaggle = df_kaggle.repartition(26)\n",
    "# df_kaggle.to_sql(name='weather_data', uri=dburi, if_exists='append',\n",
    "#                 index=False, chunksize=10000, parallel=True, method=insert_bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd226fa",
   "metadata": {},
   "source": [
    "#### Import Countries\n",
    "\n",
    "Countries will be imported as is, the schema is automatically derived by SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53e02715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:46.455692Z",
     "start_time": "2024-04-08T15:42:46.145388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 203.73 ms\n",
      "[########################################] | 100% Completed | 302.84 ms\n"
     ]
    }
   ],
   "source": [
    "countries.to_sql('countries', CONNECTION_STRING, if_exists='append',\n",
    "                 index=False, chunksize=1000, parallel=True, method=insert_bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcab082",
   "metadata": {},
   "source": [
    "#### Import Cities\n",
    "\n",
    "Cities will be imported using the updated geolocation column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1f87112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:42:48.171402Z",
     "start_time": "2024-04-08T15:42:47.862200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 202.58 ms\n",
      "[########################################] | 100% Completed | 302.57 ms\n"
     ]
    }
   ],
   "source": [
    "cities.to_sql('cities', CONNECTION_STRING, if_exists='append',\n",
    "              index=False, chunksize=1000, parallel=True, method=insert_bulk)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6501d29fd917a04d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
