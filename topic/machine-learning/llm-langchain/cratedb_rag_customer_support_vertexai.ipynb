{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "# Enhancing Customer Support with Generative AI: Applying RAG using CrateDB, LangChain and Vertex AI\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a retrieval system, which fetches\n",
    "relevant documents, with a generative model, allowing it to incorporate external\n",
    "knowledge for more accurate and informed responses.\n",
    "\n",
    "It is particularly effective for tasks like question answering, customer support,\n",
    "and any application where referencing external data can enhance the quality of the\n",
    "output.\n",
    "\n",
    "This notebook illustrates the RAG implementation of a customer support scenario.\n",
    "The corresponding dataset is based on a collection of customer support interactions\n",
    "from Twitter related to Microsoft products or services.\n",
    "\n",
    "It is derived from the modern corpus of tweets and replies published on Kaggle,\n",
    "called [Customer Support on Twitter].\n",
    "\n",
    "[Customer Support on Twitter]: https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "## What is CrateDB?\n",
    "\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "Combining RAG with CrateDB's vector store support provides a powerful framework for building sophisticated AI applications. CrateDB can store and manage the vector representations of data, which the RAG retrieval system can then utilize to fetch relevant information. Using vector search, CrateDB can quickly identify the most similar items in a large dataset based on their vector representations.\n",
    "\n",
    "\n",
    "This notebook shows how to use the CrateDB vector store functionality to create a retrieval augmented generation (RAG) pipeline. To implement RAG we use the Python client driver for CrateDB and vector store support in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open-source Python library designed to facilitate the creation and deployment of language model chains, particularly in the context of Generative AI. It provides tools for integrating various components of language models, such as retrieval systems, transformers, and custom processing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "## Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "```shell\n",
    "docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a"
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "\n",
    "# Note: If you are running in an environment like Google Colab, please use the absolute path of the requirements:\n",
    "#!pip install -r https://github.com/crate/cratedb-examples/raw/main/topic/machine-learning/llm-langchain/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import requests\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "from pueblo.util.environ import getenvpass\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import CrateDBVectorSearch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure database settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will connect to a CrateDB server instance running on localhost. You can start a sandbox instance on your workstation by running [CrateDB using Docker]. Alternatively, you can also connect to a cluster running on [CrateDB Cloud].\n",
    "\n",
    "[CrateDB Cloud]: https://console.cratedb.cloud/\n",
    "[CrateDB using Docker]: https://crate.io/docs/crate/tutorials/en/latest/basic/index.html#docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection string to running CrateDB instance.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/?schema=vertexai\",\n",
    ")\n",
    "\n",
    "# Connect to CrateDB Cloud.\n",
    "# CONNECTION_STRING = os.environ.get(\n",
    "#     \"CRATEDB_CONNECTION_STRING\",\n",
    "#     \"crate://username:password@hostname/?ssl=true&schema=vertexai\",\n",
    "# )\n",
    "\n",
    "# Define the store collection to use for this notebook session.\n",
    "COLLECTION_NAME = \"customer_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the dataset the next code snippets load dataset into a Pandas DataFrame, display the first few rows \n",
    "and show basic information such as the number of entries, column names, data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id       author_id  ...  response_tweet_id in_response_to_tweet_id\n",
      "0      2301          116231  ...               2299                  2306.0\n",
      "1     11879  MicrosoftHelps  ...                NaN                 11877.0\n",
      "2     11881  MicrosoftHelps  ...              11878                 11882.0\n",
      "3     11890          118332  ...              11889                     NaN\n",
      "4     11912  MicrosoftHelps  ...                NaN                 11911.0\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/crate/cratedb-datasets/raw/main/machine-learning/fulltext/twitter_support_microsoft.csv'\n",
    "dataset = 'twitter_support.csv'\n",
    "\n",
    "r = requests.get(url)\n",
    "with open(dataset, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 5)\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142 entries, 0 to 141\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   tweet_id                 142 non-null    int64  \n",
      " 1   author_id                142 non-null    object \n",
      " 2   inbound                  142 non-null    bool   \n",
      " 3   created_at               142 non-null    object \n",
      " 4   text                     142 non-null    object \n",
      " 5   response_tweet_id        92 non-null     object \n",
      " 6   in_response_to_tweet_id  125 non-null    float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(4)\n",
      "memory usage: 6.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG implementation with Vertex AI\n",
    "\n",
    "Vertex AI is a ML platform that combines combines data engineering, data science, and ML engineering workflows, enabling your teams to collaborate using a common toolset and scale your applications using the benefits of Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Authentication\n",
    "\n",
    "Before running AI models on Vertex AI it is necessary to authenticate to Google APIs and services.\n",
    "\n",
    "If you are running the notebook locally make sure to complete the next steps:\n",
    "- Install [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/install#installation_instructions)\n",
    "- Run the following command from command line: `gcloud auth application-default login`\n",
    "- Complete the initialization: `gcloud init`\n",
    "\n",
    "If you are running the notebook from Google Colab, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Google Cloud project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "### Create embeddings from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `CSVLoader` class to load support tickets from Twitter. The next step initializes a vector search store in CrateDB using embeddings generated by an OpenAI model. This will create a table that stores the embeddings with the name of the collection. It is important to make sure the collection name is unique and that you have the permission to create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=dataset, encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "### Ask question\n",
    "Let's define our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "### Find relevant context using similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step performs a similarity search against a collection of documents based on the given question. The search uses Eucledian distance to find similar vectors and compute the score. This returns a set of documents (`docs_with_score`) along with their corresponding similarity scores. \n",
    "\n",
    "The code then iterates over these results, and for each document (doc), it adds the content to the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [],
   "source": [
    "pattern = r\"text:\\s*(.*?)\\n(?=\\w+:)\"\n",
    "\n",
    "def return_documents(store, question):\n",
    "    # retrieve documents similar to user question\n",
    "    docs_with_score = store.similarity_search_with_score(question)\n",
    "    \n",
    "    # extract the page content\n",
    "    documents=[]\n",
    "    for doc, score in docs_with_score:\n",
    "        match = re.search(pattern, doc.page_content, re.DOTALL)\n",
    "        documents.append(match.group(1))\n",
    "    return documents\n",
    "\n",
    "documents = return_documents(store, my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "### Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step create a chatbot based on the Gemini 1.0 Pro model that serves as a customer support assistant. It relies on a  preprocessed set of documents as its knowledge base to answer questions about Microsoft products and services. This context represents the information the AI has available to answer customer questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MicrosoftHelps Is there anyway to update the shipping address on an existing Microsoft Store order? I just recently moved.---\n",
      "@MicrosoftHelps Is there anyway to update the shipping address on an existing Microsoft Store order? I just recently moved.---\n",
      "@MicrosoftHelps Seems to be good.  Support responded by email saying that the order status won't change online, but the warehouse will ship to the new addr.---\n",
      "@117762 Hellos James. You will need to cancel your current order and place a new one so you can include your updated details.\n"
     ]
    }
   ],
   "source": [
    "context = '---\\n'.join(documents)\n",
    "\n",
    "instruction = f\"\"\"You are customer support expert and get questions about Microsoft products and services. \n",
    "Answer the question by using the given context.\n",
    "Question: {my_question}\n",
    "Context: {context}\n",
    "\"\"\" \n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet is configuring a generative AI model with specific generation parameters. The choice of settings requires a careful consideration of the context in which the model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "gen_config = GenerationConfig(max_output_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next call initiates content generation with specific parameters, including a prompt (instruction) and generation configuration (gen_config). The response is expected in chunks, which are collected in a list and processed to print the complete content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will need to cancel your current order and place a new one so you can include your updated details. it seems that it's not possible to update the shipping address on an existing Microsoft Store order. The only solution provided was to cancel the current order and place a new one with the updated shipping address.\n"
     ]
    }
   ],
   "source": [
    "response = genai_model.generate_content(\n",
    "        instruction,\n",
    "        generation_config=gen_config,\n",
    "        stream=True\n",
    "    )\n",
    "response_list = []\n",
    "\n",
    "for chunk in response:\n",
    "    try:\n",
    "        response_list.append(chunk.text)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Exception occurred while calling gemini. Something is wrong. Lower the safety thresholds [safety_settings: BLOCK_NONE ] if not already done. -----\",\n",
    "            e,\n",
    "        )\n",
    "        response_list.append(\"Exception occurred\")\n",
    "        continue\n",
    "response = \"\".join(response_list)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
