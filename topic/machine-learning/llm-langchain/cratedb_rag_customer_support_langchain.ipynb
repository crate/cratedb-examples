{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "Retrieval-Augmented Generation (RAG) combines a retrieval system, which fetches relevant documents, with a generative model, allowing it to incorporate external knowledge for more accurate and informed responses. This notebook shows how to use the CrateDB vector store functionality to create a retrieval augmented generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "## What is CrateDB?\n",
    "\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "This example uses the Python client driver for CrateDB and vector store support in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "## Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "```shell\n",
    "docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain (from langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Cloning https://github.com/crate-workbench/langchain.git (to revision cratedb) to /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-install-evlqmlki/langchain_c160c6813236441ba600f33394746c79\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/crate-workbench/langchain.git /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-install-evlqmlki/langchain_c160c6813236441ba600f33394746c79\n",
      "  Resolved https://github.com/crate-workbench/langchain.git to commit 5df2429aa2fec83b424cf21bc190f8bc9c36845b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langchain-community@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/community (from -r requirements.txt (line 19))\n",
      "  Cloning https://github.com/crate-workbench/langchain.git (to revision cratedb) to /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-install-evlqmlki/langchain-community_b60d292492814c538c9138cd80449fec\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/crate-workbench/langchain.git /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-install-evlqmlki/langchain-community_b60d292492814c538c9138cd80449fec\n",
      "  Resolved https://github.com/crate-workbench/langchain.git to commit 5df2429aa2fec83b424cf21bc190f8bc9c36845b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting crash (from -r requirements.txt (line 2))\n",
      "  Using cached crash-0.30.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cratedb-toolkit==0.0.3 (from -r requirements.txt (line 4))\n",
      "  Using cached cratedb_toolkit-0.0.3-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting pueblo>=0.0.6 (from pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pueblo-0.0.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pydantic<3,>=1 (from -r requirements.txt (line 9))\n",
      "  Using cached pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pypdf<5 (from -r requirements.txt (line 10))\n",
      "  Downloading pypdf-4.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting python-dotenv<2 (from -r requirements.txt (line 11))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting requests-cache<2 (from -r requirements.txt (line 12))\n",
      "  Using cached requests_cache-1.1.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting unstructured<0.11 (from -r requirements.txt (line 13))\n",
      "  Using cached unstructured-0.10.30-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-openai==0.0.3 (from -r requirements.txt (line 14))\n",
      "  Using cached langchain_openai-0.0.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting crate[sqlalchemy] (from -r requirements.txt (line 3))\n",
      "  Using cached crate-0.35.2-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting boltons<24 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached boltons-23.1.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click<9 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting click-aliases<2,>=1.0.4 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached click_aliases-1.0.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting colorama<1 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting colorlog (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached colorlog-6.8.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting croud==1.10.0 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached croud-1.10.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting sqlalchemy (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached SQLAlchemy-2.0.25-cp38-cp38-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sqlparse<0.5 (from cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.13 (from langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Downloading langchain_core-0.1.15-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached numpy-1.24.4-cp38-cp38-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting openai<2.0.0,>=1.6.1 (from langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Downloading tiktoken-0.5.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting appdirs==1.4.4 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting bitmath==1.3.3.1 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached bitmath-1.3.3.1-py3-none-any.whl\n",
      "Collecting certifi (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting marshmallow==3.20.1 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pyyaml==6.0.1 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached PyYAML-6.0.1.tar.gz (125 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests==2.31.0 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tabulate<1.0,>=0.8 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting yarl==1.9.3 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached yarl-1.9.3-cp38-cp38-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting halo==0.0.31 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached halo-0.0.31-py3-none-any.whl\n",
      "Collecting shtab==1.6.4 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached shtab-1.6.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tqdm==4.66.1 (from croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting log-symbols>=0.0.14 (from halo==0.0.31->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
      "Collecting spinners>=0.0.24 (from halo==0.0.31->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
      "Collecting termcolor>=1.1.0 (from halo==0.0.31->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting six>=1.12.0 (from halo==0.0.31->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow==3.20.1->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests==2.31.0->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests==2.31.0->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests==2.31.0->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting multidict>=4.0 (from yarl==1.9.3->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached multidict-6.0.4-cp38-cp38-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting Pygments<3,>=2.4 (from crash->-r requirements.txt (line 2))\n",
      "  Using cached pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting platformdirs<5 (from crash->-r requirements.txt (line 2))\n",
      "  Using cached platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting prompt-toolkit<4,>=3.0 (from crash->-r requirements.txt (line 2))\n",
      "  Using cached prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting verlib2==0.2.0 (from crate[sqlalchemy]->-r requirements.txt (line 3))\n",
      "  Using cached verlib2-0.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting geojson<4,>=2.5.0 (from crate[sqlalchemy]->-r requirements.txt (line 3))\n",
      "  Using cached geojson-3.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting backports.zoneinfo<1 (from crate[sqlalchemy]->-r requirements.txt (line 3))\n",
      "  Using cached backports.zoneinfo-0.2.1.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting contextlib-chdir (from pueblo>=0.0.6->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached contextlib_chdir-1.0.2-py3-none-any.whl (2.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->-r requirements.txt (line 9))\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1->-r requirements.txt (line 9))\n",
      "  Using cached pydantic_core-2.14.6-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->-r requirements.txt (line 9))\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting attrs>=21.2 (from requests-cache<2->-r requirements.txt (line 12))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting cattrs>=22.2 (from requests-cache<2->-r requirements.txt (line 12))\n",
      "  Using cached cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting url-normalize>=1.4 (from requests-cache<2->-r requirements.txt (line 12))\n",
      "  Using cached url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting chardet (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting lxml (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached lxml-5.1.0-cp38-cp38-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Collecting nltk (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting beautifulsoup4 (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached emoji-2.10.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting dataclasses-json (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting python-iso639 (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached python_iso639-2024.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting rapidfuzz (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached rapidfuzz-3.6.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached aiohttp-3.9.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fsspec<2023.13 (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pathlibfs<0.6 (from pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pathlibfs-0.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "INFO: pip is looking at multiple versions of langchain[cratedb,openai] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting crate[sqlalchemy] (from -r requirements.txt (line 3))\n",
      "  Using cached crate-0.35.1-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached crate-0.34.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests==2.31.0->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached frozenlist-1.4.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting exceptiongroup>=1.1.1 (from cattrs>=22.2->requests-cache<2->-r requirements.txt (line 12))\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting adlfs (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached adlfs-2023.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting smbprotocol (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached smbprotocol-1.12.0-py3-none-any.whl.metadata (13 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gcsfs (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached gcsfs-2023.12.2.post1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting dask (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached dask-2023.5.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting distributed (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached distributed-2023.5.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting s3fs (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pygit2 (from fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pygit2-1.13.3-cp38-cp38-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->langchain[cratedb,openai]@ git+https://github.com/crate-workbench/langchain.git@cratedb#subdirectory=libs/langchain->-r requirements.txt (line 18))\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.13->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.6.1->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.6.1->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.6.1->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting wcwidth (from prompt-toolkit<4,>=3.0->crash->-r requirements.txt (line 2))\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.6.0,>=0.5.2->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached regex-2023.12.25-cp38-cp38-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting joblib (from nltk->unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai==0.0.3->-r requirements.txt (line 14))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured<0.11->-r requirements.txt (line 13))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting azure-core<2.0.0,>=1.23.1 (from adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached azure_core-1.29.7-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46 (from adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
      "Collecting azure-identity (from adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached azure_identity-1.15.0-py3-none-any.whl.metadata (75 kB)\n",
      "Collecting azure-storage-blob>=12.12.0 (from adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached azure_storage_blob-12.19.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting cloudpickle>=1.5.0 (from dask->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting partd>=1.2.0 (from dask->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached partd-1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting toolz>=0.10.0 (from dask->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jinja2>=2.10.3 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting locket>=1.0.0 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting msgpack>=1.0.0 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached msgpack-1.0.7-cp38-cp38-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting psutil>=5.7.0 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tornado>=6.0.3 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached tornado-6.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting zict>=2.2.0 (from distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Collecting decorator>4.1.2 (from gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth>=1.2 (from gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_auth-2.26.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib (from gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-storage (from gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl.metadata (6.1 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cffi>=1.16.0 (from pygit2->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached cffi-1.16.0.tar.gz (512 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached aiobotocore-2.11.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting cryptography>=2.0 (from smbprotocol->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached cryptography-42.0.0-cp37-abi3-macosx_10_12_universal2.whl.metadata (5.3 kB)\n",
      "Collecting pyspnego (from smbprotocol->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pyspnego-0.10.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting botocore<1.34.23,>=1.33.2 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached botocore-1.34.22-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting msal<2,>=1.16.0 (from azure-datalake-store<0.1,>=0.0.46->adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached msal-1.26.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate>=0.6.1 (from azure-storage-blob>=12.12.0->adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting pycparser (from cffi>=1.16.0->pygit2->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.2->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.2->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.10.3->distributed->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached MarkupSafe-2.1.4-cp38-cp38-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity->adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached google_crc32c-1.5.0-cp38-cp38-macosx_10_9_universal2.whl (32 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.34.23,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.34.23,>=1.33.2->aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests==2.31.0->croud==1.10.0->cratedb-toolkit==0.0.3->-r requirements.txt (line 4))\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.46->adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity->adlfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->fsspec[abfs,dask,gcs,git,github,http,s3,smb]<2023.13; extra == \"fileio\"->pueblo[cli,fileio,nlp]>=0.0.6->-r requirements.txt (line 8))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached cratedb_toolkit-0.0.3-py3-none-any.whl (76 kB)\n",
      "Using cached langchain_openai-0.0.3-py3-none-any.whl (28 kB)\n",
      "Using cached croud-1.10.0-py2.py3-none-any.whl (107 kB)\n",
      "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached yarl-1.9.3-cp38-cp38-macosx_11_0_arm64.whl (81 kB)\n",
      "Using cached crash-0.30.2-py2.py3-none-any.whl (36 kB)\n",
      "Using cached crate-0.34.0-py2.py3-none-any.whl (117 kB)\n",
      "Using cached pueblo-0.0.6-py3-none-any.whl (27 kB)\n",
      "Using cached pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.6-cp38-cp38-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Downloading pypdf-4.0.0-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.9/283.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached requests_cache-1.1.1-py3-none-any.whl (60 kB)\n",
      "Using cached unstructured-0.10.30-py3-none-any.whl (1.7 MB)\n",
      "Using cached aiohttp-3.9.1-cp38-cp38-macosx_11_0_arm64.whl (388 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached boltons-23.1.1-py2.py3-none-any.whl (195 kB)\n",
      "Using cached cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached click_aliases-1.0.4-py3-none-any.whl (3.2 kB)\n",
      "Using cached colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Using cached geojson-3.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_core-0.1.15-py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Using cached openai-1.9.0-py3-none-any.whl (223 kB)\n",
      "Using cached pathlibfs-0.5.0-py3-none-any.whl (11 kB)\n",
      "Using cached platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Using cached prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached SQLAlchemy-2.0.25-cp38-cp38-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tiktoken-0.5.2-cp38-cp38-macosx_11_0_arm64.whl (955 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.9/955.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached emoji-2.10.0-py2.py3-none-any.whl (457 kB)\n",
      "Using cached lxml-5.1.0-cp38-cp38-macosx_10_9_universal2.whl (6.2 MB)\n",
      "Using cached python_iso639-2024.1.2-py3-none-any.whl (274 kB)\n",
      "Using cached rapidfuzz-3.6.1-cp38-cp38-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Using cached anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached frozenlist-1.4.1-cp38-cp38-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached regex-2023.12.25-cp38-cp38-macosx_11_0_arm64.whl (291 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached adlfs-2023.12.0-py3-none-any.whl (38 kB)\n",
      "Using cached dask-2023.5.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached distributed-2023.5.0-py3-none-any.whl (966 kB)\n",
      "Using cached gcsfs-2023.12.2.post1-py2.py3-none-any.whl (34 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached pygit2-1.13.3-cp38-cp38-macosx_10_9_universal2.whl (5.8 MB)\n",
      "Using cached s3fs-2023.12.2-py3-none-any.whl (28 kB)\n",
      "Using cached smbprotocol-1.12.0-py3-none-any.whl (124 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Using cached aiobotocore-2.11.0-py3-none-any.whl (76 kB)\n",
      "Using cached azure_core-1.29.7-py3-none-any.whl (192 kB)\n",
      "Using cached azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp38-cp38-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached cryptography-42.0.0-cp37-abi3-macosx_10_12_universal2.whl (5.9 MB)\n",
      "Using cached google_auth-2.26.2-py2.py3-none-any.whl (186 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached msgpack-1.0.7-cp38-cp38-macosx_11_0_arm64.whl (231 kB)\n",
      "Using cached partd-1.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl (249 kB)\n",
      "Using cached tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tornado-6.4-cp38-abi3-macosx_10_9_universal2.whl (433 kB)\n",
      "Using cached azure_identity-1.15.0-py3-none-any.whl (164 kB)\n",
      "Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "Using cached pyspnego-0.10.2-py3-none-any.whl (129 kB)\n",
      "Using cached botocore-1.34.22-py3-none-any.whl (11.9 MB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached MarkupSafe-2.1.4-cp38-cp38-macosx_10_9_universal2.whl (17 kB)\n",
      "Using cached msal-1.26.0-py2.py3-none-any.whl (99 kB)\n",
      "Using cached msal_extensions-1.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: pyyaml, langchain, langchain-community, backports.zoneinfo, cffi\n",
      "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-6.0.1-cp38-cp38-macosx_10_14_arm64.whl size=45368 sha256=4171575d07f0a39fc06547912a39c664d30a2bf67eae05e63173eeecb0f9762c\n",
      "  Stored in directory: /Users/marijaselakovic/Library/Caches/pip/wheels/77/54/77/68b3079bd1d88cb070513c3935d9f7e32c70ad69368375308d\n",
      "  Building wheel for langchain (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langchain: filename=langchain-0.1.1-py3-none-any.whl size=803905 sha256=1e315cd51f99ba7ea516f706f64528c912cf8595cfefe7ffab6df06c0ed7e00c\n",
      "  Stored in directory: /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-ephem-wheel-cache-dull03p5/wheels/2b/16/e6/8c02139b37dbda9ad0fa1d37364c421a177658247868a95a52\n",
      "  Building wheel for langchain-community (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langchain-community: filename=langchain_community-0.0.13-py3-none-any.whl size=1576811 sha256=85a0ae14e51245935790676c0a0d453a68e3dc58cd56f8011c30015124964822\n",
      "  Stored in directory: /private/var/folders/3f/htk34xrs62d0jxkjddpz35qc0000gn/T/pip-ephem-wheel-cache-dull03p5/wheels/dc/9d/65/a3c68c557b348ea7a0704e2f3f1c80b0de5d5f6d1aa8f8645c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for backports.zoneinfo (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for backports.zoneinfo: filename=backports.zoneinfo-0.2.1-cp38-cp38-macosx_10_14_arm64.whl size=49463 sha256=33d2689fb8e1dc6f40f256f99f1ce9878eb7e0be087ddee9f8bdeaf9301e6195\n",
      "  Stored in directory: /Users/marijaselakovic/Library/Caches/pip/wheels/c7/de/cc/c405827ed64f81b56142f1e0075a970b2731b00d21983d54fb\n",
      "  Building wheel for cffi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cffi: filename=cffi-1.16.0-cp38-cp38-macosx_10_14_arm64.whl size=263440 sha256=987f0cb4782782ed22f47a2339f58226a388af9e849d55efc8db78489b049056\n",
      "  Stored in directory: /Users/marijaselakovic/Library/Caches/pip/wheels/f4/df/d7/20c740c0373c550cdca4fcf0eb9af36c769ad8553ea81c6a2f\n",
      "Successfully built pyyaml langchain langchain-community backports.zoneinfo cffi\n",
      "Installing collected packages: wcwidth, spinners, sortedcontainers, filetype, boltons, bitmath, appdirs, zipp, zict, wrapt, urllib3, typing-extensions, tqdm, tornado, toolz, termcolor, tenacity, tblib, tabulate, sqlparse, soupsieve, sniffio, six, shtab, regex, rapidfuzz, pyyaml, python-magic, python-iso639, python-dotenv, PyJWT, Pygments, pycparser, pyasn1, psutil, protobuf, prompt-toolkit, portalocker, platformdirs, packaging, oauthlib, numpy, mypy-extensions, multidict, msgpack, MarkupSafe, lxml, locket, jsonpointer, joblib, jmespath, idna, h11, google-crc32c, geojson, fsspec, frozenlist, exceptiongroup, emoji, distro, decorator, contextlib-chdir, colorlog, colorama, cloudpickle, click, charset-normalizer, chardet, certifi, cachetools, backports.zoneinfo, backoff, attrs, async-timeout, yarl, url-normalize, typing-inspect, sqlalchemy, rsa, requests, python-dateutil, pypdf, pydantic-core, pyasn1-modules, pueblo, pathlibfs, partd, nltk, marshmallow, log-symbols, langdetect, jsonpatch, jinja2, isodate, importlib-metadata, httpcore, googleapis-common-protos, google-resumable-media, crate, click-aliases, cffi, cattrs, beautifulsoup4, anyio, annotated-types, aiosignal, aioitertools, tiktoken, requests-oauthlib, requests-cache, pygit2, pydantic, httpx, halo, google-auth, dataclasses-json, dask, cryptography, crash, botocore, azure-core, aiohttp, unstructured, pyspnego, openai, langsmith, google-auth-oauthlib, google-api-core, distributed, croud, azure-storage-blob, aiobotocore, smbprotocol, s3fs, msal, langchain-core, google-cloud-core, cratedb-toolkit, msal-extensions, langchain-openai, langchain-community, google-cloud-storage, azure-datalake-store, langchain, gcsfs, azure-identity, adlfs\n",
      "Successfully installed MarkupSafe-2.1.4 PyJWT-2.8.0 Pygments-2.17.2 adlfs-2023.12.0 aiobotocore-2.11.0 aiohttp-3.9.1 aioitertools-0.11.0 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.2.0 appdirs-1.4.4 async-timeout-4.0.3 attrs-23.2.0 azure-core-1.29.7 azure-datalake-store-0.0.53 azure-identity-1.15.0 azure-storage-blob-12.19.0 backoff-2.2.1 backports.zoneinfo-0.2.1 beautifulsoup4-4.12.3 bitmath-1.3.3.1 boltons-23.1.1 botocore-1.34.22 cachetools-5.3.2 cattrs-23.2.3 certifi-2023.11.17 cffi-1.16.0 chardet-5.2.0 charset-normalizer-3.3.2 click-8.1.7 click-aliases-1.0.4 cloudpickle-3.0.0 colorama-0.4.6 colorlog-6.8.0 contextlib-chdir-1.0.2 crash-0.30.2 crate-0.34.0 cratedb-toolkit-0.0.3 croud-1.10.0 cryptography-42.0.0 dask-2023.5.0 dataclasses-json-0.6.3 decorator-5.1.1 distributed-2023.5.0 distro-1.9.0 emoji-2.10.0 exceptiongroup-1.2.0 filetype-1.2.0 frozenlist-1.4.1 fsspec-2023.12.2 gcsfs-2023.12.2.post1 geojson-3.1.0 google-api-core-2.15.0 google-auth-2.26.2 google-auth-oauthlib-1.2.0 google-cloud-core-2.4.1 google-cloud-storage-2.14.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.62.0 h11-0.14.0 halo-0.0.31 httpcore-1.0.2 httpx-0.26.0 idna-3.6 importlib-metadata-7.0.1 isodate-0.6.1 jinja2-3.1.3 jmespath-1.0.1 joblib-1.3.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.13 langchain-core-0.1.15 langchain-openai-0.0.3 langdetect-1.0.9 langsmith-0.0.83 locket-1.0.0 log-symbols-0.0.14 lxml-5.1.0 marshmallow-3.20.1 msal-1.26.0 msal-extensions-1.1.0 msgpack-1.0.7 multidict-6.0.4 mypy-extensions-1.0.0 nltk-3.8.1 numpy-1.24.4 oauthlib-3.2.2 openai-1.9.0 packaging-23.2 partd-1.4.1 pathlibfs-0.5.0 platformdirs-4.1.0 portalocker-2.8.2 prompt-toolkit-3.0.43 protobuf-4.25.2 psutil-5.9.8 pueblo-0.0.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pycparser-2.21 pydantic-2.5.3 pydantic-core-2.14.6 pygit2-1.13.3 pypdf-4.0.0 pyspnego-0.10.2 python-dateutil-2.8.2 python-dotenv-1.0.1 python-iso639-2024.1.2 python-magic-0.4.27 pyyaml-6.0.1 rapidfuzz-3.6.1 regex-2023.12.25 requests-2.31.0 requests-cache-1.1.1 requests-oauthlib-1.3.1 rsa-4.9 s3fs-2023.12.2 shtab-1.6.4 six-1.16.0 smbprotocol-1.12.0 sniffio-1.3.0 sortedcontainers-2.4.0 soupsieve-2.5 spinners-0.0.24 sqlalchemy-2.0.25 sqlparse-0.4.4 tabulate-0.9.0 tblib-3.0.0 tenacity-8.2.3 termcolor-2.4.0 tiktoken-0.5.2 toolz-0.12.1 tornado-6.4 tqdm-4.66.1 typing-extensions-4.9.0 typing-inspect-0.9.0 unstructured-0.10.30 url-normalize-1.4.3 urllib3-1.26.18 wcwidth-0.2.13 wrapt-1.16.0 yarl-1.9.3 zict-3.0.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import warnings\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import CrateDBVectorSearch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure database settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will connect to a CrateDB server instance running on localhost. You can start a sandbox instance on your workstation by running [CrateDB using Docker]. Alternatively, you can also connect to a cluster running on [CrateDB Cloud].\n",
    "\n",
    "[CrateDB Cloud]: https://console.cratedb.cloud/\n",
    "[CrateDB using Docker]: https://crate.io/docs/crate/tutorials/en/latest/basic/index.html#docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection string to running CrateDB instance.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/\",\n",
    ")\n",
    "\n",
    "# Define the store collection to use for this notebook session.\n",
    "COLLECTION_NAME = \"customer_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example you need to have an API key from OpenAI. This is typically done by creating an account on OpenAI's website and accessing the API section, where you can generate a new key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key:········\n"
     ]
    }
   ],
   "source": [
    "from pueblo.util.environ import getenvpass\n",
    "\n",
    "getenvpass(\"OPENAI_API_KEY\", prompt=\"OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patches\n",
    "Those can be removed again after they have been upstreamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cratedb_toolkit.sqlalchemy.patch import patch_inspector\n",
    "patch_inspector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "## Create embeddings from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `CSVLoader` class to load support tickets from Twitter. The next step initializes a vector search store in CrateDB using embeddings generated by an OpenAI model. This will create a table that stores the embeddings with the name of the collection. Make sure the collection name is unique and that you have the permission to create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./sample_data/twitter_support_microsoft.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "## Ask question\n",
    "Let's define our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "## Find relevant context using similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity search uses Eucledian distance to find similar vectors and compute the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MicrosoftHelps Is there anyway to update the shipping address on an existing Microsoft Store order? I just recently moved.\n",
      "@MicrosoftHelps Is there anyway to update the shipping address on an existing Microsoft Store order? I just recently moved.\n",
      "@MicrosoftHelps Seems to be good.  Support responded by email saying that the order status won't change online, but the warehouse will ship to the new addr.\n",
      "@118333 2/2 Store app or via Microsoft Store online?\n"
     ]
    }
   ],
   "source": [
    "docs_with_score = store.similarity_search_with_score(my_question)\n",
    "documents=[]\n",
    "pattern = r\"text: (.+)\\nresponse_tweet_id:\"\n",
    "for doc, score in docs_with_score:\n",
    "    match = re.search(pattern, doc.page_content, re.DOTALL)\n",
    "    if match:\n",
    "        documents.append(match.group(1).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "## Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step we create an interactive chatbot scenario where GPT-4 serves as a customer support assistant, using a given set of documents as its knowledge base to answer questions about Microsoft products and services. If the answer to a question isn't in the provided documents, it's programmed to respond with \"I don't know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IEuq9r2EaqUz"
   },
   "outputs": [],
   "source": [
    "context = '---\\n'.join(documents)\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are customer support expert and get questions about Microsoft products and services.\n",
    "To answer question use the information from the context. Remove new line characters from the answer.\n",
    "If you don't find the relevant information there, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "chat_completion = openai.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                               messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                         {\"role\": \"user\", \"content\": my_question}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aQnmpCIZa13L",
    "outputId": "a10c71a7-a6c7-4f83-c069-df49703d4654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To update the shipping address on an existing order in the Microsoft Store, you can either contact Microsoft Support via email or update the shipping address in the Microsoft Store app or Microsoft Store online. The support team will inform the warehouse of the new address for shipping.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
