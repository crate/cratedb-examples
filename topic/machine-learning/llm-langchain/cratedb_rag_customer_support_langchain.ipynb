{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "# Enhancing Customer Support with Generative AI: Applying RAG using CrateDB and LangChain\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a retrieval system, which fetches\n",
    "relevant documents, with a generative model, allowing it to incorporate external\n",
    "knowledge for more accurate and informed responses.\n",
    "\n",
    "It is particularly effective for tasks like question answering, customer support,\n",
    "and any application where referencing external data can enhance the quality of the\n",
    "output.\n",
    "\n",
    "This notebook illustrates the RAG implementation of a customer support scenario.\n",
    "The corresponding dataset is based on a collection of customer support interactions\n",
    "from Twitter related to Microsoft products or services.\n",
    "\n",
    "It is derived from the modern corpus of tweets and replies published on Kaggle,\n",
    "called [Customer Support on Twitter].\n",
    "\n",
    "[Customer Support on Twitter]: https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "## What is CrateDB?\n",
    "\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "Combining RAG with CrateDB's vector store support provides a powerful framework for building sophisticated AI applications. CrateDB can store and manage the vector representations of data, which the RAG retrieval system can then utilize to fetch relevant information. Using vector search, CrateDB can quickly identify the most similar items in a large dataset based on their vector representations.\n",
    "\n",
    "\n",
    "This notebook shows how to use the CrateDB vector store functionality to create a retrieval augmented generation (RAG) pipeline. To implement RAG we use the Python client driver for CrateDB and vector store support in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open-source Python library designed to facilitate the creation and deployment of language model chains, particularly in the context of Generative AI. It provides tools for integrating various components of language models, such as retrieval systems, transformers, and custom processing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "## Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "```shell\n",
    "docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "\n",
    "# Note: If you are running in an environment like Google Colab, please use the absolute path of the requirements:\n",
    "#!pip install -r https://raw.githubusercontent.com/crate/cratedb-examples/main/topic/machine-learning/llm-langchain/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "from pueblo.util.environ import getenvpass\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import CrateDBVectorSearch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure database settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will connect to a CrateDB server instance running on localhost. You can start a sandbox instance on your workstation by running [CrateDB using Docker]. Alternatively, you can also connect to a cluster running on [CrateDB Cloud].\n",
    "\n",
    "[CrateDB Cloud]: https://console.cratedb.cloud/\n",
    "[CrateDB using Docker]: https://crate.io/docs/crate/tutorials/en/latest/basic/index.html#docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection string to running CrateDB instance.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/?schema=openai\",\n",
    ")\n",
    "\n",
    "# Connect to CrateDB Cloud.\n",
    "# CONNECTION_STRING = os.environ.get(\n",
    "#     \"CRATEDB_CONNECTION_STRING\",\n",
    "#     \"crate://username:password@hostname/?ssl=true&schema=notebook\",\n",
    "# )\n",
    "\n",
    "# Define the store collection to use for this notebook session.\n",
    "COLLECTION_NAME = \"customer_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the dataset the next code snippets load dataset into a Pandas DataFrame, display the first few rows \n",
    "and show basic information such as the number of entries, column names, data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id       author_id  ...  response_tweet_id in_response_to_tweet_id\n",
      "0      2301          116231  ...               2299                  2306.0\n",
      "1     11879  MicrosoftHelps  ...                NaN                 11877.0\n",
      "2     11881  MicrosoftHelps  ...              11878                 11882.0\n",
      "3     11890          118332  ...              11889                     NaN\n",
      "4     11912  MicrosoftHelps  ...                NaN                 11911.0\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/crate/cratedb-datasets/raw/main/machine-learning/fulltext/twitter_support_microsoft.csv'\n",
    "dataset = 'twitter_support.csv'\n",
    "\n",
    "r = requests.get(url)\n",
    "with open(dataset, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 5)\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142 entries, 0 to 141\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   tweet_id                 142 non-null    int64  \n",
      " 1   author_id                142 non-null    object \n",
      " 2   inbound                  142 non-null    bool   \n",
      " 3   created_at               142 non-null    object \n",
      " 4   text                     142 non-null    object \n",
      " 5   response_tweet_id        92 non-null     object \n",
      " 6   in_response_to_tweet_id  125 non-null    float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(4)\n",
      "memory usage: 6.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG implementation with OpenAI and CrateDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "getenvpass(\"OPENAI_API_KEY\", prompt=\"OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "### Create embeddings from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `CSVLoader` class to load support tickets from Twitter. The next step initializes a vector search store in CrateDB using embeddings generated by an OpenAI model. This will create a table that stores the embeddings with the name of the collection. It is important to make sure the collection name is unique and that you have the permission to create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=dataset, encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "### Ask question\n",
    "Let's define our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\"\n",
    "\n",
    "# Alternative question.\n",
    "# my_question = \"I can not make purchase on Xbox for fifa points, what to do?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "### Find relevant context using similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step performs a similarity search against a collection of documents based on the given question. The search uses Eucledian distance to find similar vectors and compute the score. This returns a set of documents (`docs_with_score`) along with their corresponding similarity scores. \n",
    "\n",
    "The code then iterates over these results, and for each document (doc), it adds the content to the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [],
   "source": [
    "def return_documents(store, question):\n",
    "    # retrieve documents similar to user question\n",
    "    docs_with_score = store.similarity_search_with_score(question)\n",
    "    \n",
    "    # extract the page content\n",
    "    documents=[]\n",
    "    for doc, score in docs_with_score:\n",
    "        documents.append(doc.page_content)\n",
    "    return documents\n",
    "\n",
    "documents = return_documents(store, my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "### Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step we create an interactive chatbot scenario where GPT-3.5-turbo serves as a customer support assistant, using a preprocessed set of documents as its knowledge base to answer questions about Microsoft products and services. This context represents the information the AI has available to answer customer questions. A `system_prompt` is then constructed, instructing the AI that it is a customer support expert specializing in Microsoft products and services. The prompt also specifies that if the answer to a question isn't in the provided documents, the system should respond with \"I don't know.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(documents):\n",
    "    context = '---\\n'.join(documents)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are customer support expert and get questions about Microsoft products and services.\n",
    "    To answer question use the information from the context. Remove new line characters from the answer.\n",
    "    If you don't find the relevant information in the context, say \"I don't know\".\n",
    "\n",
    "    Context:\n",
    "    {context}\"\"\"\n",
    "    \n",
    "    return system_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEuq9r2EaqUz"
   },
   "outputs": [],
   "source": [
    "system_prompt = create_prompt(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question we need an interactive chatbot scenario where GPT-3.5 or another LLM serves as a customer support assistant, using a given set of documents and system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = openai.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                               messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                         {\"role\": \"user\", \"content\": my_question}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to access the content response message generated by the OpenAI model in the context of a chat conversation we need to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aQnmpCIZa13L",
    "outputId": "a10c71a7-a6c7-4f83-c069-df49703d4654"
   },
   "outputs": [],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrateDB &#129505; OpenSource\n",
    "\n",
    "In the second part of this notebook we build a Retrieval-Augmented Generation (RAG) system using CrateDB with Jina AI and the Llama open-source models. \n",
    "\n",
    "### Configure Jina AI API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jina AI embeddings on Hugging Face are a suite of open-source models designed for encoding text data into high-dimensional vectors, which can be used in various natural language processing tasks such as semantic search, text classification, and clustering. These embeddings utilize the BERT architecture and are specially tuned for longer sequence lengths up to 8192 tokens, which is notably higher than the standard BERT model's capacity.\n",
    "\n",
    "To use Jina AI embeddings you need a Jina AI API key which can be easily obtained once you sign up for an account on the Jina AI website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "\n",
    "getenvpass(\"JINA_API_KEY\", prompt=\"Jina API key:\")\n",
    "embeddings = JinaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize new embedding store\n",
    "\n",
    "The next step initializes a vector search store in CrateDB using embeddings generated by Jina AI models and returns relevant documents using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/jina\",\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"customer_data_jina\"\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "documents = return_documents(store, my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to a local LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest ways to connect to a local LLM is to download one of the models from the [Llamafile repository](https://github.com/Mozilla-Ocho/llamafile#llamafile) and run it locally. Upon initiating the llamafile, it not only sets up a web-based user interface for chatting at the local address http://127.0.0.1:8080/, but it also offers an endpoint for chat completions that is compatible with the OpenAI API.\n",
    "\n",
    "In this example, we use the Phi-2 model that is about 2GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# An API key is not required!\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\", \n",
    "    api_key = \"sk-no-key-required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a system prompt using the previously defined template and instantiate a client to interact with the \"LLaMA_CPP\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = create_prompt(documents)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": my_question}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion, we extract the response message generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
