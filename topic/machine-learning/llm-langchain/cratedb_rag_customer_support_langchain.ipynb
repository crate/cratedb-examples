{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "Retrieval-Augmented Generation (RAG) combines a retrieval system, which fetches relevant documents, with a generative model, allowing it to incorporate external knowledge for more accurate and informed responses. This notebook shows how to use the CrateDB vector store functionality to create a retrieval augmented generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "## What is CrateDB?\n",
    "\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "This example uses the Python client driver for CrateDB and vector store support in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "## Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "```shell\n",
    "docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import warnings\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import CrateDBVectorSearch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure database settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will connect to a CrateDB server instance running on localhost. You can start a sandbox instance on your workstation by running [CrateDB using Docker]. Alternatively, you can also connect to a cluster running on [CrateDB Cloud].\n",
    "\n",
    "[CrateDB Cloud]: https://console.cratedb.cloud/\n",
    "[CrateDB using Docker]: https://crate.io/docs/crate/tutorials/en/latest/basic/index.html#docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection string to running CrateDB instance.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/\",\n",
    ")\n",
    "\n",
    "# Define the store collection to use for this notebook session.\n",
    "COLLECTION_NAME = \"customer_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example you need to have an API key from OpenAI. This is typically done by creating an account on OpenAI's website and accessing the API section, where you can generate a new key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pueblo.util.environ import getenvpass\n",
    "\n",
    "getenvpass(\"OPENAI_API_KEY\", prompt=\"OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patches\n",
    "Those can be removed again after they have been upstreamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cratedb_toolkit.sqlalchemy.patch import patch_inspector\n",
    "patch_inspector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "## Create embeddings from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `CSVLoader` class to load support tickets from Twitter. The next step initializes a vector search store in CrateDB using embeddings generated by an OpenAI model. This will create a table that stores the embeddings with the name of the collection. Make sure the collection name is unique and that you have the permission to create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./sample_data/twitter_support_microsoft.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "## Ask question\n",
    "Let's define our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "## Find relevant context using similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity search uses Eucledian distance to find similar vectors and compute the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [],
   "source": [
    "docs_with_score = store.similarity_search_with_score(my_question)\n",
    "documents=[]\n",
    "pattern = r\"text: (.+)\\nresponse_tweet_id:\"\n",
    "for doc, score in docs_with_score:\n",
    "    match = re.search(pattern, doc.page_content, re.DOTALL)\n",
    "    if match:\n",
    "        documents.append(match.group(1).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "## Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step we create an interactive chatbot scenario where GPT-4 serves as a customer support assistant, using a given set of documents as its knowledge base to answer questions about Microsoft products and services. If the answer to a question isn't in the provided documents, it's programmed to respond with \"I don't know.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEuq9r2EaqUz"
   },
   "outputs": [],
   "source": [
    "context = '---\\n'.join(documents)\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are customer support expert and get questions about Microsoft products and services.\n",
    "To answer question use the information from the context. Remove new line characters from the answer.\n",
    "If you don't find the relevant information there, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "chat_completion = openai.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                               messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                         {\"role\": \"user\", \"content\": my_question}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aQnmpCIZa13L",
    "outputId": "a10c71a7-a6c7-4f83-c069-df49703d4654"
   },
   "outputs": [],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
