{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "# Enhancing Customer Support with Generative AI: Applying RAG using CrateDB and LangChain\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a retrieval system, which fetches\n",
    "relevant documents, with a generative model, allowing it to incorporate external\n",
    "knowledge for more accurate and informed responses.\n",
    "\n",
    "It is particularly effective for tasks like question answering, customer support,\n",
    "and any application where referencing external data can enhance the quality of the\n",
    "output.\n",
    "\n",
    "This notebook illustrates the RAG implementation of a customer support scenario.\n",
    "The corresponding dataset is based on a collection of customer support interactions\n",
    "from Twitter related to Microsoft products or services.\n",
    "\n",
    "It is derived from the modern corpus of tweets and replies published on Kaggle,\n",
    "called [Customer Support on Twitter].\n",
    "\n",
    "[Customer Support on Twitter]: https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "## What is CrateDB?\n",
    "\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "Combining RAG with CrateDB's vector store support provides a powerful framework for building sophisticated AI applications. CrateDB can store and manage the vector representations of data, which the RAG retrieval system can then utilize to fetch relevant information. Using vector search, CrateDB can quickly identify the most similar items in a large dataset based on their vector representations.\n",
    "\n",
    "\n",
    "This notebook shows how to use the CrateDB vector store functionality to create a retrieval augmented generation (RAG) pipeline. To implement RAG we use the Python client driver for CrateDB and vector store support in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangChain?\n",
    "\n",
    "LangChain is an open-source Python library designed to facilitate the creation and deployment of language model chains, particularly in the context of Generative AI. It provides tools for integrating various components of language models, such as retrieval systems, transformers, and custom processing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "## Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "```shell\n",
    "docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "\n",
    "# Note: If you are running in an environment like Google Colab, please use the absolute path of the requirements:\n",
    "#!pip install -r https://raw.githubusercontent.com/crate/cratedb-examples/main/topic/machine-learning/llm-langchain/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "from pueblo.util.environ import getenvpass\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import CrateDBVectorSearch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure database settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will connect to a CrateDB server instance running on localhost. You can start a sandbox instance on your workstation by running [CrateDB using Docker]. Alternatively, you can also connect to a cluster running on [CrateDB Cloud].\n",
    "\n",
    "[CrateDB Cloud]: https://console.cratedb.cloud/\n",
    "[CrateDB using Docker]: https://crate.io/docs/crate/tutorials/en/latest/basic/index.html#docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection string to running CrateDB instance.\n",
    "CONNECTION_STRING = os.environ.get(\n",
    "    \"CRATEDB_CONNECTION_STRING\",\n",
    "    \"crate://crate@localhost/\",\n",
    ")\n",
    "\n",
    "# Connect to CrateDB Cloud.\n",
    "# CONNECTION_STRING = os.environ.get(\n",
    "#     \"CRATEDB_CONNECTION_STRING\",\n",
    "#     \"crate://username:password@hostname/?ssl=true&schema=notebook\",\n",
    "# )\n",
    "\n",
    "# Define the store collection to use for this notebook session.\n",
    "COLLECTION_NAME = \"customer_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example you need to have an API key from OpenAI. This is typically done by creating an account on OpenAI's website and accessing the API section, where you can generate a new key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getenvpass(\"OPENAI_API_KEY\", prompt=\"OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the dataset the next code snippets load dataset into a Pandas DataFrame, display the first few rows \n",
    "and show basic information such as the number of entries, column names, data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/crate/cratedb-datasets/raw/main/machine-learning/fulltext/twitter_support_microsoft.csv'\n",
    "dataset = 'twitter_support.csv'\n",
    "\n",
    "r = requests.get(url)\n",
    "with open(dataset, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 5)\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "## Create embeddings from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `CSVLoader` class to load support tickets from Twitter. The next step initializes a vector search store in CrateDB using embeddings generated by an OpenAI model. This will create a table that stores the embeddings with the name of the collection. It is important to make sure the collection name is unique and that you have the permission to create a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=dataset, encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "# Alternative: HuggingFace embeddings\n",
    "\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = CrateDBVectorSearch.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=data,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "## Ask question\n",
    "Let's define our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\"\n",
    "\n",
    "# Alternative question.\n",
    "# my_question = \"I can not make purchase on Xbox for fifa points, what to do?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "## Find relevant context using similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step performs a similarity search against a collection of documents based on the given question. The search uses Eucledian distance to find similar vectors and compute the score. This returns a set of documents (`docs_with_score`) along with their corresponding similarity scores. \n",
    "\n",
    "The code then iterates over these results, and for each document (doc), it adds the content to the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [],
   "source": [
    "# retrieve documents similar to user question\n",
    "docs_with_score = store.similarity_search_with_score(my_question)\n",
    "\n",
    "# extract the page content\n",
    "documents=[]\n",
    "for doc, score in docs_with_score:\n",
    "    documents.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "## Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step we create an interactive chatbot scenario where GPT-3.5-turbo serves as a customer support assistant, using a preprocessed set of documents as its knowledge base to answer questions about Microsoft products and services. This context represents the information the AI has available to answer customer questions. A `system_prompt` is then constructed, instructing the AI that it is a customer support expert specializing in Microsoft products and services. The prompt also specifies that if the answer to a question isn't in the provided documents, the system should respond with \"I don't know.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEuq9r2EaqUz"
   },
   "outputs": [],
   "source": [
    "context = '---\\n'.join(documents)\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are customer support expert and get questions about Microsoft products and services.\n",
    "To answer question use the information from the context. Remove new line characters from the answer.\n",
    "If you don't find the relevant information in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question we need an interactive chatbot scenario where GPT-3.5 or another LLM serves as a customer support assistant, using a given set of documents and system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = openai.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                               messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                         {\"role\": \"user\", \"content\": my_question}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to access the content response message generated by the OpenAI model in the context of a chat conversation we need to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aQnmpCIZa13L",
    "outputId": "a10c71a7-a6c7-4f83-c069-df49703d4654"
   },
   "outputs": [],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
