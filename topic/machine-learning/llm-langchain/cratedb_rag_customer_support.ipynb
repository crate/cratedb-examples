{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUPQQ-jlMkUd"
   },
   "source": [
    "This notebook shows how to use the CrateDB vector store functionality around FLOAT_VECTOR and KNN_MATCH. You will learn how to use it to create a retrieval augmented generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe-5yxFDMl0S"
   },
   "source": [
    "#What is CrateDB?\n",
    "CrateDB is an open-source, distributed, and scalable SQL analytics database for storing and analyzing massive amounts of data in near real-time, even with complex queries. It is wire-compatible to PostgreSQL, based on Lucene, and inherits the shared-nothing distribution layer of Elasticsearch.\n",
    "\n",
    "This example uses the Python client driver for CrateDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-UtZJnMs2q"
   },
   "source": [
    "#Getting Started\n",
    "CrateDB supports storing vectors since version 5.5. You can leverage the fully managed service of CrateDB Cloud, or install CrateDB on your own, for example using Docker.\n",
    "\n",
    "`docker run --publish 4200:4200 --publish 5432:5432 --pull=always crate:latest -Cdiscovery.type=single-node`\n",
    "\n",
    "Install required Python packages, and import Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJyP1GEXNHUy",
    "outputId": "9c62258f-f6a1-4578-ced4-40f15f586e9a"
   },
   "outputs": [],
   "source": [
    "pip install langchain pypdf chromadb openai sentence_transformers sqlalchemy 'crate[sqlalchemy]' tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUNjBDrXNNoG"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import crate\n",
    "import openai\n",
    "import os\n",
    "from pueblo.util.environ import getenvpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd2BLNlReU01"
   },
   "source": [
    "# Create embeddings from dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsRfzgmeNjJc",
    "outputId": "6fdff9a4-8007-4773-818d-2cf2fcce28b9"
   },
   "outputs": [],
   "source": [
    "getenvpass(\"OPENAI_API_KEY\", prompt=\"OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Po5rpReNuhn",
    "outputId": "84e363de-84be-4c96-d3b7-8c4561fd03db"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./sample_data/twitter_support_microsoft.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()\n",
    "pages_text = [doc.page_content for doc in data]\n",
    "print(pages_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWl5RSPjPgGv"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(deployment='my-embedding-model', chunk_size=1)\n",
    "pages_embeddings = embeddings.embed_documents(pages_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhOU-4aXQkTX"
   },
   "source": [
    "#Write data to CrateDB\n",
    "\n",
    "The next step creates a dataframe that contains the text of the documents and their embeddings. The embeddings will be stored in CrateDB using FLOAT_VECTOR type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_I0dlUNQgKU"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pages_text, pages_embeddings)),columns =['text', 'embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzsx_YdaV2et",
    "outputId": "221fe177-212e-4e74-9a26-e4d48d6eae3d"
   },
   "outputs": [],
   "source": [
    "host = getpass.getpass(\"Host:\")\n",
    "password = getpass.getpass(\"password:\")\n",
    "dbname=\"crate://admin:{0}@{1}:4200?ssl=true\".format(password,host)\n",
    "create_table = text(\"CREATE TABLE text_data (text TEXT, embedding FLOAT_VECTOR(1536))\")\n",
    "engine = create_engine(dbname, echo=False)\n",
    "\n",
    "with engine.connect() as con:\n",
    "     con.execute(create_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5MDKdW5Y_Um"
   },
   "source": [
    "The text and embeddings are written to CrateDB database using CrateDB vector storage support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8pzKlr3uV3Ql",
    "outputId": "93566fa0-7ef4-44fd-e29e-23e7b29c645f"
   },
   "outputs": [],
   "source": [
    "df.to_sql(name='text_data', con=engine, if_exists='append', index=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkAPZ55RZQ09"
   },
   "source": [
    "#Ask question\n",
    "Let's define our question and create an embedding using OpenAI embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InhR73isZJCB"
   },
   "outputs": [],
   "source": [
    "my_question = \"How to update shipping address on existing order in Microsoft Store?\"\n",
    "query_embedding = embeddings.embed_query(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnNZHI6ajaS"
   },
   "source": [
    "#Find relevant context using similarity search\n",
    "\n",
    "The `knn_match (search_vector, query_vector, k) `function in CrateDB performs an approximate k-nearest neighbors (KNN) search within a dataset. KNN search involves finding the k data points that are most similar to a given query data point. We find the most similar vectors to our query vector using knn search capability in CrateDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjLeMkwMagOf",
    "outputId": "2c92d6fc-22aa-4914-b58c-bbd3928108e1"
   },
   "outputs": [],
   "source": [
    "knn_query = text(\"\"\"SELECT text FROM text_data\n",
    "            WHERE knn_match(embedding, {0}, 2)\"\"\".format(query_embedding))\n",
    "documents=[]\n",
    "\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(knn_query)\n",
    "    for record in results:\n",
    "        documents.append(record[0])\n",
    "\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j94BF-3e1Je"
   },
   "source": [
    "#Augment system prompt and query LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEuq9r2EaqUz"
   },
   "outputs": [],
   "source": [
    "context = '---\\n'.join(documents)\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are customer support expert and get questions about Microsoft products and services.\n",
    "To answer question use the information from the context. Remove new line characters from the answer.\n",
    "If you don't find the relevant information there, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "chat_completion = openai.chat.completions.create(model=\"gpt-4\",\n",
    "                                               messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                                         {\"role\": \"user\", \"content\": my_question}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aQnmpCIZa13L",
    "outputId": "a10c71a7-a6c7-4f83-c069-df49703d4654"
   },
   "outputs": [],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTGxziOBvK25"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
